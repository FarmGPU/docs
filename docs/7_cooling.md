---
sidebar_position: 7
---

**C. Cooling Systems Evolution: From Air to Advanced Liquid Cooling**

The immense heat generated by densely packed AI accelerators renders traditional air cooling methods increasingly inadequate. While air cooling (using Computer Room Air Conditioners/Handlers \- CRACs/CRAHs, chillers, cooling towers, and economization techniques) remains viable for lower-density traditional data centers (typically below 20 kW/rack) 8, it struggles to cope efficiently with the thermal loads of AI hardware.5 The industry average Power Usage Effectiveness (PUE) for air-cooled facilities is around 1.6, although optimized hyperscale designs achieve much better figures (1.1-1.15).8

For AI data centers, the transition to liquid cooling is becoming essential.4 Key liquid cooling technologies include:

1. **Direct-to-Chip (DLC) Liquid Cooling:** This method involves circulating a liquid coolant (often treated water or an engineered fluid) through cold plates mounted directly onto the primary heat-generating components like GPUs and CPUs.28 Heat is efficiently transferred from the chip to the liquid, which is then pumped away to be cooled, typically via Coolant Distribution Units (CDUs) that interface with facility water loops.8 DLC is considered the leading solution for current and upcoming high-density AI racks, such as NVIDIA's 120kW NVL72 platform, which mandates DLC.8 Microsoft is also deploying DLC, including its "sidekick" system, for its custom Maia AI accelerators.13
2. **Immersion Cooling:** This more radical approach involves fully submerging server components or entire servers in a non-conductive, dielectric fluid.28 It offers potentially higher cooling efficiency by eliminating the need for server fans and allowing direct fluid contact with all components. It exists in single-phase (fluid remains liquid) and two-phase (fluid boils on hot surfaces, transferring heat via vaporization and condensation) forms.28 While promising, immersion cooling faces challenges related to fluid compatibility with materials, servicing complexity, standardization, and potentially higher upfront costs.30 The OCP is actively working on standards to address these challenges across fluid handling, materials compatibility, reliability, and power distribution within tanks.33

Liquid cooling offers significant potential for energy efficiency improvements at high densities. By directly capturing heat at the source, it allows facility water loops to operate at higher temperatures (e.g., 32°C or even 40°C, compared to \~7°C in some older air-cooled systems).8 This higher temperature facilitates more efficient heat exchange and significantly increases the opportunities for "free cooling" (using ambient air or water to cool the facility loop without energy-intensive chillers) and makes waste heat reuse more practical and economically viable.8 Google, for example, achieves PUE values around 1.1 using highly optimized water-side economizer techniques in its large-scale facilities.8 However, directly comparing PUE between air-cooled and liquid-cooled systems can be complex due to how server fan power is accounted for in the metric.8 Diagrams illustrating various air and liquid cooling architectures can be found in resources like Semianalysis's Datacenter Anatomy Part 2\.8

The extreme power densities inherent in AI hardware are forcing a fundamental shift in data center design philosophy. No longer can power, cooling, and rack infrastructure be treated as largely independent systems. Instead, a holistic, co-designed approach is becoming necessary. Traditional data centers could often separate bulk power distribution, zoned air cooling, and standardized IT racks.14 However, managing heat loads of 60-120kW+ within a single rack footprint demands highly localized and efficient thermal management.5 Liquid cooling solutions, particularly DLC and immersion, require the integration of coolant distribution manifolds, pumps, CDUs, and potentially specialized power delivery mechanisms (like bus bars for immersion tanks 34) directly within or immediately adjacent to the IT racks.8 This tight integration necessitates designing the rack, its power feed, and its cooling loop as an interconnected system from the outset. This often leads to deviations from standard 19-inch rack form factors, as seen in custom designs like Microsoft's Ares racks for its Maia accelerators.31 This co-design imperative significantly increases complexity and favors purpose-built AI facilities or extensive, costly retrofits over simple, incremental upgrades of traditional spaces.5

Furthermore, the colossal power appetite of large-scale AI data centers is reshaping site selection priorities. Historically, factors like low-latency network access (crucial for retail colocation 4) or the availability of large, scalable building shells (important for wholesale providers 4) were primary drivers. However, AI training, the initial major driver of dedicated AI data center construction, is relatively insensitive to network latency but exceptionally demanding in terms of power consumption and reliability.6 A single large AI training cluster can require hundreds of megawatts, potentially straining local electrical grids.9 Consequently, locations offering access to abundant, reliable, and cost-effective power – potentially from large hydroelectric dams, nuclear power plants, or significant renewable energy installations – are becoming prime targets for AI data center development, even if they are geographically remote from major population or network hubs.6 This focus on power availability could lead to a geographic clustering of AI training capacity in specific power-rich regions, diverging significantly from the distribution patterns of traditional data centers, which tend to cluster near urban centers and major network peering points.6

Finally, the widespread adoption of liquid cooling introduces new operational paradigms and challenges distinct from the well-established practices for air-cooled environments. While air cooling maintenance typically involves routine tasks like changing air filters, inspecting fans, and standard HVAC system checks 8, liquid cooling demands a different set of skills and procedures. Operators must manage coolant quality, monitoring parameters like pH balance, conductivity, and potential biological growth, and implementing treatments like corrosion inhibitors or biocides.36 Robust leak detection systems and protocols are essential, as coolant leaks can be far more damaging to IT equipment than air circulation issues.32 Maintenance routines must include inspections of pumps, valves, cold plates, manifolds, and CDUs.32 Immersion cooling adds further complexity regarding fluid handling, material compatibility over time, and safe servicing procedures.30 Failures within liquid cooling loops can have rapid and severe consequences for the equipment being cooled, potentially offering shorter ride-through times compared to air systems if operating close to thermal limits.29 This necessitates specialized training for data center technicians, updated safety protocols, and potentially closer partnerships with cooling system vendors for maintenance and support, impacting operational expenditures and requiring a shift in operational mindset.32